{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "#Functionalities\n",
    "from collections import Counter\n",
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#NLP\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Custom Transformer\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.preprocess.preprocessor import TextPreprocessor\n",
    "from src.models.train import train_model, optimize_model\n",
    "\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from src.models.metrics import evaluate_model \n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "We will once again be loading our data in order to further optimize our models (LightGBM and SGD)\n",
    "\n",
    "The decision for the best combination of hyperparameters will be taken via GridSearchCV (faster than RandomizedSearchCV)\n",
    "Optimization will be performed using function $optimize_model$, which also saves the trained model into the $models$ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/processed/fake_or_real_news_clean.csv')\n",
    "\n",
    "# train test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df['text_clean'], df['label'], test_size=0.2, random_state=0, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "We are modifying the next hyperparameters:\n",
    "- Ngram_range: whether we use onegram, bigram or trigram vectorization\n",
    "- Min_df & max_df: whether or not we include words that appear too often or too little in our data for vectorization\n",
    "- Learning rate\n",
    "- Number of estimators (+Estimator +Model-complexity)\n",
    "- Importance type: what algorithm should be used in order to determine tree divisions\n",
    "- Number of leaves for each branch of our decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 22702.06s\n",
      "ROC-AUC score of the model: 0.9792119268336176\n",
      "Accuracy of the model: 0.9318541996830428\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       631\n",
      "           1       0.95      0.92      0.93       631\n",
      "\n",
      "    accuracy                           0.93      1262\n",
      "   macro avg       0.93      0.93      0.93      1262\n",
      "weighted avg       0.93      0.93      0.93      1262\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[598  33]\n",
      " [ 53 578]]\n",
      "\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# parameter combinations\n",
    "param_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2), (1,3)],\n",
    "    \"vectorizer__min_df\": [0.01, 0],\n",
    "    \"vectorizer__max_df\": [0.99, 1],\n",
    "    'clf__n_estimators' : [200, 300, 400], \n",
    "    'clf__importance_type' : ['split', 'gain'],\n",
    "    'clf__num_leaves': [7, 14, 21, 28, 31, 50],\n",
    "    'clf__learning_rate': [0.1, 0.03, 0.003]\n",
    "}\n",
    "\n",
    "optimize_model(clf=LGBMClassifier(), param_grid=param_grid, scoring='accuracy', cv=3, xtrain=xtrain, xtest=xtest, ytrain=ytrain, ytest=ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "We are modifying the next hyperparameters:\n",
    "- Ngram_range\n",
    "- Min_df & max_df\n",
    "- Loss normalization technique: whether we use l1 or l2 normalization\n",
    "- Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Proba not available\n",
      "Execution time: 206.86s\n",
      "Accuracy of the model: 0.9429477020602218\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       631\n",
      "           1       0.94      0.94      0.94       631\n",
      "\n",
      "    accuracy                           0.94      1262\n",
      "   macro avg       0.94      0.94      0.94      1262\n",
      "weighted avg       0.94      0.94      0.94      1262\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[596  35]\n",
      " [ 37 594]]\n",
      "\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"vectorizer__min_df\": [0.0],\n",
    "    \"vectorizer__max_df\": [1.0],\n",
    "    \"vectorizer__stop_words\": ['english'],\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2), (1,3)],\n",
    "    \"vectorizer__norm\": [\"l1\", \"l2\"],\n",
    "    \"clf__loss\": [\"hinge\", \"log\"]\n",
    "}\n",
    "\n",
    "optimize_model(SGDClassifier(), param_grid=param_grid, scoring='accuracy', cv=3, xtrain=xtrain, xtest=xtest, ytrain=ytrain, ytest=ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "The most valuable algorithm when it comes to predictions is SGD. However, for the sake of entertainment and more interesting interpretability we are choosing the $LightGBM$ model to move past this stage.\n",
    "Again, SGD can only do so much for us when it comes to interpretability due to it not allowing us to understand the way it outputs probabilities. (This is, $predict_proba$ can't be done)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "143932acd3e23b30a25a992805630cc3d7427a5d92f8535ce47afb800650a272"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
