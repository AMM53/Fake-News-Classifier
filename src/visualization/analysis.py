#Basics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scikitplot as skplt
import pickle

#Functionalities
from collections import Counter
import sys, os
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split

# Explicability
import shap

# Model
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.pipeline import Pipeline 
from sklearn.model_selection import GridSearchCV
from lightgbm import LGBMClassifier

# Métrics
from sklearn.metrics import (roc_curve, roc_auc_score, f1_score , confusion_matrix, recall_score, 
                             precision_score, classification_report, precision_recall_curve, accuracy_score)
from lightgbm import plot_importance

from ..preprocess.preprocessor import TextPreprocessor
from ..models.train import train_model, optimize_model
from ..models.metrics import evaluate_model

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import os
import string
import re
import nltk
from nltk.util import ngrams
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
import spacy
from spacy import displacy
import matplotlib.pyplot as plt


def model_analysis(model, ypred, ypred_proba, ytest):

    evaluate_model(ypred=ypred, ypred_proba=ypred_proba, ytest=ytest)
    skplt.metrics.plot_confusion_matrix(ytest,ypred,normalize=True,figsize=(10, 8),)
    skplt.metrics.plot_roc(ytest, ypred_proba, figsize=(10,8))
    skplt.metrics.plot_cumulative_gain(ytest, ypred_proba, figsize=(10, 8))
    skplt.metrics.plot_lift_curve(ytest, ypred_proba, figsize=(10, 8))
    skplt.metrics.plot_precision_recall(ytest, ypred_proba, figsize=(10, 8))

    # Feature names generated by TFIDF
    fn = model.best_estimator_.named_steps['vectorizer'].get_feature_names()
    # Feature Importance in LGBM
    fi = model.best_estimator_._final_estimator.feature_importances_

    feature_imp = pd.DataFrame(sorted(zip(fi,fn)), columns=['Value','Feature'])

    plt.figure(figsize=(20, 20))
    sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False)[0:100])
    plt.title(f'{model.__class__.__name__} Feature Importance')
    plt.tight_layout()
    plt.show()

# check na values by column
def null_per_column(df):
    nulos_col = pd.merge(df.isnull().sum().sort_values().to_frame('missing_values_n').reset_index(),
            df.dtypes.to_frame('feature_type').reset_index(),
            on = 'index',
            how = 'inner')
    nulos_col['column_pct'] = nulos_col['missing_values_n']/df.shape[0]
    return nulos_col.sort_values(['missing_values_n', 'feature_type', 'column_pct'], ascending=False)


# check duplicates
def check_duplicates(df, glimpse=False):
    print('--------------------')
    print(f'{(df.duplicated().sum())} duplicate values')
    print(f'{round((df.duplicated().sum())/len(df)*100, 4)}% of data')
    print('--------------------')

    # Glimpse allows us to have a brief look at
    # the duplicate values for a certain column
    if glimpse!=False:
        return df[df[glimpse].duplicated(keep=False)].sort_values(glimpse).head(10)


def plot_word_number_histogram(text):
    text.str.split().map(lambda x: len(x)).plot(kind='hist', bins=50).set_title('Word Count Histogram')
    plt.show()


def frequency_analysis(df, column, target, binary=True):
    true = df[df[target]==1][column]
    fake = df[df[target]==0][column]

    text_true = ' '.join(true)
    text_fake = ' '.join(fake)
    text = ' '.join(df[column])

    pd.Series(Counter(word_tokenize(text))).sort_values(ascending=False).iloc[:30].plot(kind="bar").set_title('Top 30 words in all news')
    plt.show()
    pd.Series(Counter(word_tokenize(text_fake))).sort_values(ascending=False).iloc[:30].plot(kind="bar", color="red").set_title('Top 30 words in fake news')
    plt.show()
    pd.Series(Counter(word_tokenize(text_true))).sort_values(ascending=False).iloc[:30].plot(kind="bar", color="green").set_title('Top 30 words in real news')
    plt.show()

    return 

def wordcloud_plot(df, column, target, color_fondo='white', color_palabras='winter_r', binary=True, which='all'):
    if binary:
        true = df[df[target]==1][column]
        fake = df[df[target]==0][column]

        text_true = ' '.join(true)
        text_fake = ' '.join(fake)
        text = ' '.join(df[column])

    if which=='all':
        wordcloud = WordCloud(width=3000, 
                            height=2000, 
                            random_state=0, 
                            background_color=color_fondo, 
                            colormap='turbo_r',
                            collocations=False).generate(text) 
        plt.imshow(wordcloud) 

    if which=='true':
        wordcloud1 = WordCloud(width=3000, 
                            height=2000, 
                            random_state=0, 
                            background_color=color_fondo, 
                            colormap='winter_r',
                            collocations=False).generate(text_true) 
        plt.imshow(wordcloud1) 

    if which=='fake':
        wordcloud2 = WordCloud(width=3000, 
                            height=2000, 
                            random_state=0, 
                            background_color=color_fondo, 
                            colormap='autumn_r',
                            collocations=False).generate(text_fake) 
        plt.imshow(wordcloud2) 

        
def eda(df, column, target, binary=True):
    text = df[column]
    plot_word_number_histogram(df[column])
    frequency_analysis(df, column, target)
    ngrams_frequency(df)

# ngrams
def ngrams_frequency(df):
    all_text = ' '.join(df['text_clean'])
    bigram_freq = nltk.FreqDist(nltk.bigrams(word_tokenize(all_text)))
    bigram_freq.plot(30, cumulative=False,title='Top 30 Most Common Bigrams')

    trigram_freq = nltk.FreqDist(nltk.trigrams(word_tokenize(all_text)))
    trigram_freq.plot(15, cumulative=False,title='Top 15 Most Common Trigrams')


# Clean text

# strip spaces
def strip_spaces(text):
    return text.strip()

# remove uppercase letters
def remove_upper(text):
    return text.lower()

# clean html residue
def clean_html(text):
    text = re.sub(r'<.*?>', '', text)
    return text

# clean urls
def clean_url(text):
    text = re.sub(r'http\S+', 'url', text)
    return text

# clean newlines
def clean_newline(text):
    text = text.replace('\n', ' ')
    return text

# clean punctuation
def clean_punctuation(text):
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

# clean numbers
def clean_numbers(text):
    text = re.sub(r'\d+', '', text)
    return text

# clean quotes 
def clean_quotes(text):
    return text.replace('’', '').replace('“', '').replace('”','').replace('‘','').replace(' — ',' ')

# remove stopwords
def clean_stopwords(text):
    stop_words = set(nltk.corpus.stopwords.words('english'))
    text = [word for word in text if word not in stop_words]
    text = ' '.join(text)
    return text

# lemmatize text
def lemmatize(text):
    lemmatizer = nltk.WordNetLemmatizer()
    text = [lemmatizer.lemmatize(word,pos='v') for word in text.split()]
    return text

# preprocessing function
def preprocessing(text): 
    text = strip_spaces(text)
    text = remove_upper(text)
    text = clean_html(text)
    text = clean_url(text)
    text = clean_newline(text)
    text = clean_punctuation(text)
    text = clean_numbers(text)
    text = clean_quotes(text)
    text = lemmatize(text)
    text = clean_stopwords(text)

    return text